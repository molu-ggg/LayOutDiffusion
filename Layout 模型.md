
# SD
åœ¨æ¨ç†é˜¶æ®µï¼Œstable diffusion æ˜¯ éšæœºå™ªå£° + text prompt å åå‘æ¢å¤
# è®ºæ–‡é˜…è¯»
## é¢†åŸŸä¿¡æ¯
ä»€ä¹ˆæ˜¯ï¼šLayoutï¼Ÿ<br />é€šå¸¸æŒ‡çš„æ˜¯ä»…é¢„æµ‹å‡ºæ¯ä¸€å¼ å›¾ç‰‡å‡ ä¸ªå…ƒç´ çš„æœ€ä½³çš„ç›¸å¯¹ä½ç½®ï¼ˆtype+bounding boxes)ã€‚ä¹Ÿæœ‰ä¸€äº›è®ºæ–‡æ˜¯ç”Ÿæˆå›¾ç‰‡ã€‚<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/29540509/1717987411829-a45745c0-e2ee-4c9e-b445-2d055a4a6726.png#averageHue=%23a8a6a2&clientId=u7e84b3ce-c973-4&from=paste&height=442&id=u389f0655&originHeight=884&originWidth=1182&originalType=binary&ratio=2&rotation=0&showTitle=false&size=473973&status=done&style=none&taskId=uc62aa97e-3caa-4f6b-8d33-c480c4c862a&title=&width=591)<br />è¿™ç±»æ–¹æ³•çš„ä¸»è¦ä»»åŠ¡å¯ä»¥åˆ†æˆ:<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/29540509/1717987759964-760a1362-3c41-4603-acf0-3718271b00f4.png#averageHue=%23e2e1df&clientId=u7e84b3ce-c973-4&from=paste&height=212&id=ucb2aabfe&originHeight=424&originWidth=918&originalType=binary&ratio=2&rotation=0&showTitle=false&size=83271&status=done&style=none&taskId=u0c2c8a74-4b95-4c5b-aaf7-000b3e51ce0&title=&width=459)<br />1.ä»…æä¾›æ ‡ç­¾ï¼Œç”Ÿæˆæ¯ä¸€ä¸ªå…ƒç´ çš„åæ ‡<br />2.æ ‡ç­¾+å¤§å°ï¼ˆé•¿å®½ï¼‰ï¼Œ ç”Ÿæˆæ¯ä¸€ä¸ªå…ƒç´ çš„åæ ‡<br />3.æ ‡ç­¾+ è¯­è¨€æè¿°ç›¸å¯¹ä½ç½®ç­‰ä¿¡æ¯ï¼Œ ç”Ÿæˆæ¯ä¸€ä¸ªå…ƒç´ çš„åæ ‡<br />4.å¯¹å½“å‰çš„æ ‡ç­¾+åæ ‡åšæ›´åˆç†çš„å¸ƒå±€ï¼Œç”Ÿæˆæ¯ä¸€ä¸ªå…ƒç´ çš„åæ ‡<br />5.ç»™å®šä¸€ä¸ªå…ƒç´ çš„æ ‡ç­¾+åæ ‡ï¼Œå¡«å……å…¶ä»–å…ƒç´ ï¼Œç”Ÿæˆåˆé€‚å…ƒç´ ç±»å‹+åæ ‡<br />6.æ²¡æœ‰æä¾›ä»»ä½•å†…å®¹ï¼Œç”Ÿæˆåˆé€‚å…ƒç´ ç±»å‹+åæ ‡<br />å…¶å®æœ¬è´¨æ¥è¯´ï¼Œè¿™æ˜¯CVä»»åŠ¡ç”¨NLPæ–¹æ³•å»åšï¼š<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/29540509/1717988290996-cbf138b5-46f0-4377-95ad-589c93baeef0.png#averageHue=%23e3e3e3&clientId=u7e84b3ce-c973-4&from=paste&height=215&id=ue7e3537a&originHeight=430&originWidth=706&originalType=binary&ratio=2&rotation=0&showTitle=false&size=73299&status=done&style=none&taskId=u8226eb17-f599-4913-8c7c-edea7d88d82&title=&width=353)

## LayerOutVAE
## LayerGAN
![image.png](https://cdn.nlark.com/yuque/0/2024/png/29540509/1717987440922-393e80ee-3837-4027-8ed1-ac3ee42ccc40.png#averageHue=%23f2f0ee&clientId=u7e84b3ce-c973-4&from=paste&height=397&id=ue832a3bd&originHeight=794&originWidth=1396&originalType=binary&ratio=2&rotation=0&showTitle=false&size=581983&status=done&style=none&taskId=u1e9f04da-1704-46f7-a4e5-d135db69d00&title=&width=698)<br />**å¦‚æœæœ‰æ—¶é—´ï¼Œå¯ä»¥çœ‹çœ‹è¿™ç¯‡äº†è®ºæ–‡**ï¼ˆæœ‰å¿…è¦ï¼ï¼‰<br />è¿™ç¯‡è®ºæ–‡æ¯”è¾ƒç–‘æƒ‘çš„æ˜¯ï¼š<br />ä¸ºä»€ä¹ˆè¾“å…¥æ˜¯ï¼šéšæœºæ¯ä¸ªå…ƒç´ éƒ½æœ‰ä¸€ç»„å‡ ä½•å‚æ•°Î¸å’Œä¸€ä¸ªç±»æ¦‚ç‡å‘é‡pã€‚è¿™æ˜¯è®­ç»ƒè¿‡ç¨‹å—ï¼Ÿ<br />å¦‚æœæ¶‰åŠåˆ°ä¸åŒé£æ ¼çš„ç…§ç‰‡ï¼Œä»–æ˜¯æ€ä¹ˆæ ¹æ®ä¸åŒç±»åˆ«çš„æ ‡ç­¾çš„ä¿¡æ¯è¿›è¡Œç”Ÿæˆçš„ï¼Ÿ<br />**è¾“å…¥ï¼šå‡åŒ€åˆ†å¸ƒå’Œé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„å…·æœ‰éšæœºç±»æ¦‚ç‡å’Œå‡ ä½•å‚æ•°çš„ä¸€ç»„å›¾å½¢å…ƒç´ **
## LayerOutGAN++
Constrained Graphic Layout Generation via Latent Optimization<br />å¸ƒå±€å¯¹å›¾åƒè®¾è®¡å’Œåœºæ™¯ç”Ÿæˆéå¸¸é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºLayoutGAN çš„[ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ](https://so.csdn.net/so/search?q=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&spm=1001.2101.3001.7020)ï¼Œç§°ä¸ºLayout GAN++ï¼Œå®ƒé€šè¿‡å»ºæ¨¡ä¸åŒç±»å‹çš„2Då…ƒç´ çš„å‡ ä½•å…³ç³»æ¥ç»¼åˆ**å¸ƒå±€**ã€‚<br />ä¸»è¦å†…å®¹æ˜¯ï¼š<br />ç®—æ³•çš„æ¡†æ¶ï¼š<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/29540509/1717987484426-f9ea68aa-4217-4bb7-afc6-92416c40682f.png#averageHue=%23f3f0ef&clientId=u7e84b3ce-c973-4&from=paste&height=395&id=u71969edd&originHeight=790&originWidth=2516&originalType=binary&ratio=2&rotation=0&showTitle=false&size=531535&status=done&style=none&taskId=u1239ca11-3b25-4ba3-a3d7-1e6b7f0a76d&title=&width=1258)<br />G çš„è¾“å…¥ï¼š å™ªéŸ³ + labels è¾“å‡ºï¼šbounding boxes B<br />D : generated bounding boxes ğµ and conditional labels ğ¿.è¾“å‡ºæ˜¯ä¸€ä¸ªæ ‡é‡å€¼ï¼Œå®ƒé‡åŒ–äº†å¸ƒå±€çš„çœŸå®æ€§ï¼Œå¹¶å°è¯•ä»å†…éƒ¨è¡¨ç¤ºä¸­é‡å»ºç»™å®šçš„è¾¹ç•Œæ¡†ï¼Œè¾“å‡ºçš„å‘é‡è¿˜æ˜¯æ¯”è¾ƒé•¿çš„<br />Aux : ä½œè€…ç»éªŒå‘ç°ï¼Œåœ¨æ–‡æ¡£å¯¹é½è‰¯å¥½çš„ä¸å±…ä¸­ï¼Œåˆ¤æ–­åˆ«è¢«è®­ç»ƒä¸ºå¯¹å¯¹é½çš„æ•æ„Ÿï¼Œå¹¶ä¸”å¯¹ä½ç½®çš„è¶‹åŠ¿ä¸å¤ªæ•æ„Ÿï¼Œå°±åªæ˜¯å…³ç³»ç‰©ä½“æ˜¯å¦å¯¹é½ï¼Œä½†æ˜¯ä¸å…³å¿ƒä¸å¯»å¸¸çš„å¸ƒå±€ã€‚<br />è¿™é‡Œçš„**å¯¹é½**æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ å°±æ˜¯æŸä¸ªç‰©ä½“æ˜¯å¦åœ¨ç”»é¢ä¸­å‡ºç°è¿‡å—ï¼Ÿ<br />ç–‘é—®ï¼š<br />1.è¾“å…¥æ˜¯ï¼š éšæœºå™ªå£°+ ä¸åŒæ¨¡æ€çš„æ•°æ®çš„labels(a conditional multiset of labels L )ï¼Ÿ ï¼ˆwhy??ï¼‰

1. è¿™ä¸ªä»–æ²¡æï¼Œéœ€è¦çœ‹ä»£ç ï¼Œå¤§æ¦‚ç‡ä¸æ˜¯åŒä¸€ä¸ªï¼Ÿï¼Ÿï¼Ÿ
2. æˆ‘è®¤ä¸ºä¹Ÿä¸æ˜¯åŒä¸€ä¸ªï¼Ÿï¼Ÿï¼Ÿ

4.è¿™ä¸ªéœ€è¦è¿›ä¸€æ­¥äº†è§£ï¼Ÿï¼Ÿï¼Ÿ<br />5.è¿™é‡Œæ˜¯ä¸æ˜¯åˆ¤æ–­è¾“å…¥çš„æ˜¯Gç”Ÿæˆçš„ è¿˜æ˜¯ ç›´æ¥ç¼–ç çš„ ï¼Ÿ è²Œä¼¼ ä¸å¤ªå¯¹<br />**å®éªŒçš„è¯„ä¼°æŒ‡æ ‡ï¼š**<br />[FID](https://www.baidu.com/s?wd=FID&rsv_idx=2&tn=baiduhome_pg&usm=1&ie=utf-8&rsv_pq=a954b521000e13d6&oq=FID%20%E6%8C%87%E6%A0%87&rsv_t=8becgk7WWFWoceInhWifIVJldggef%2BbPwawB90TKMB26aetWyWDqWjOjtmyjp7YKVNE1&sa=re_dqa_zy&icon=1)[î›ª](https://www.baidu.com/s?wd=FID&rsv_idx=2&tn=baiduhome_pg&usm=1&ie=utf-8&rsv_pq=a954b521000e13d6&oq=FID%20%E6%8C%87%E6%A0%87&rsv_t=8becgk7WWFWoceInhWifIVJldggef%2BbPwawB90TKMB26aetWyWDqWjOjtmyjp7YKVNE1&sa=re_dqa_zy&icon=1)_ï¼ˆFrÃ©chet Inception Distanceï¼‰æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°ç”Ÿæˆæ¨¡å‹å’ŒçœŸå®æ•°æ®åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æŒ‡æ ‡ å¦‚ä½•å³å¯ç®—ï¼š_2ä¸ªåˆ†å¸ƒä¹‹é—´çš„**FrÃ©chetè·ç¦»**æ¥è¡¡é‡[ç”Ÿæˆæ¨¡å‹](https://so.csdn.net/so/search?q=%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B&spm=1001.2101.3001.7020)å’ŒçœŸå®æ•°æ®åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚FrÃ©chetè·ç¦»æ˜¯ä¸€ç§åº¦é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´è·ç¦»çš„æ–¹æ³•ï¼Œå®ƒè€ƒè™‘åˆ°äº†ä¸¤ä¸ªåˆ†å¸ƒçš„å‡å€¼å’Œ[åæ–¹å·®çŸ©é˜µ](https://so.csdn.net/so/search?q=%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5&spm=1001.2101.3001.7020)ï¼Œå¯ä»¥æ›´å¥½åœ°æè¿°ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚<br />FID^2 = ||\mu_1 - \mu_2||^2 + Tr(\Sigma_1 + \Sigma_2 - 2(\Sigma_1\Sigma_2)^{1/2})$<br />Maximum IoU: ä¸¤ä¸ªç”Ÿæˆçš„å¸ƒå±€å’Œå‚è€ƒé›†åˆä¹‹é—´å®šä¹‰ã€‚<br />Alignment and overlap ï¼šè¿™ä¸ªæŒ‡æ ‡æ˜¯ä»–ä»¬ä¹‹å‰çš„å·¥ä½œæå‡ºæ¥çš„ï¼Œå¯ä»¥çœ‹çœ‹ä»–ä»¬çš„LayoutGAN
## LayoutDiffusion
LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models<br />**è¿™ä¸ªæ–¹æ³•è·ŸLayerGAN æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ**<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/29540509/1717987523351-19bc3dfd-8f14-4d03-b037-4be3122e3e25.png#averageHue=%23f1ebe7&clientId=u7e84b3ce-c973-4&from=paste&height=441&id=u02f6c321&originHeight=882&originWidth=2540&originalType=binary&ratio=2&rotation=0&showTitle=false&size=924810&status=done&style=none&taskId=u1daf7a6f-6c7b-4966-b3c3-9c824aa430e&title=&width=1270)

### ä¸»è¦æ€æƒ³ï¼š
é‡‡ç”¨äº†diffusionçš„æ–¹æ³•ï¼š<br />1.forward processï¼šåæ ‡è¢«è½»å¾®ç ´åä¸ºå¹³ç¨³åˆ†å¸ƒï¼Œ type tokensåœ¨**åæœŸ**è¢«å¸æ”¶åˆ°MASKä¸­ã€‚<br />2.reverse process: types é¦–å…ˆè¢«æ¢å¤ï¼Œç„¶åå¯¹åæ ‡é€æ¸ç»†åŒ–<br />è¿™ä¸ªè¿‡ç¨‹æ˜¯åæ ‡ç»†åŒ–çš„è¿‡ç¨‹ï¼Œå¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œå¯ä»¥**æ§åˆ¶è¾“å…¥**çš„ä¸åŒæ¥å®ç°ä¸åŒçš„ä»»åŠ¡ï¼Œå› ä¸ºä»–ä»¬æœ€ç»ˆçš„è¾“å‡ºéƒ½æ˜¯ä¸€æ ·çš„ã€‚

### ä¸»è¦è´¡çŒ®ï¼š
1.è®¾è®¡çš„ç®—æ³•åªæ˜¯èƒ½å¤Ÿåœ¨ type elements å†…éƒ¨ æˆ–è€… åæ ‡å†…éƒ¨ä¹‹é—´è½¬æ¢<br />2.è®¾è®¡äº†è¿‘ç«¯è½¬æ¢ï¼Œæ›´å€¾å‘äºè½¬æ¢åˆ°ç›¸é‚»åæ ‡<br />3.é’ˆå¯¹typeï¼Œè®¾è®¡äº†æ‰€æœ‰å…ƒç´ æœ‰ä¸€å®šæ¦‚ç‡åˆ°maskè½¬æ¢ï¼Œä¸”é™åˆ¶åœ¨diffusionæœ€åä¸€ä¸ªé˜¶æ®µã€‚<br />4.å³æ’å³ç”¨ï¼Œå³æ§åˆ¶åå‘ä¹‹å‰çš„è¾“å…¥ä¿¡æ¯ï¼Œå³å¯å®Œæˆä¸åŒçš„ä»»åŠ¡ã€‚


### æ¨¡å‹è¯¦æƒ…
åœ¨åå‘è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ˜¯Transformerï¼Œæ¨¡å‹è¾“å…¥çš„æ•°æ®æ„é€ æ˜¯ï¼š xtçš„ç¼–ç å‘é‡ï¼Œä½ç½®ç¼–ç ï¼Œæ—¶é—´tç¼–ç 


ä»training set's prior distribution ä¸­sampling ä¸€ä¸ªnï¼Œ<br />å¯¹äºæ„é€ xTï¼Œæˆ‘ä»¬ä¸ºç±»å‹åˆ†é…MASKæ ‡è®°ï¼Œå¹¶ä¸ºå‰nä¸ªå…ƒç´ çš„è¾¹ç•Œæ¡†åˆ†é…éšæœºåæ ‡æ ‡è®°ã€‚å¯¹äºå‰©ä½™çš„(Nâˆ’n)ä¸ªå…ƒç´ ï¼Œä½¿ç”¨PADä»¤ç‰Œæ¥ç¡®ä¿é•¿åº¦ä¸€è‡´.
### ä»£ç è§£è¯»

ç¡®å®štraining_dataï¼Œä¹Ÿå°±æ˜¯TextDataset 
```python
###- æ¨¡å‹çš„è¾“å…¥ï¼š training_data 
def get_corpus_rocstory(data_args, model, seq_length, padding_mode='block',
                        split='train', load_vocab=None):
    print("_______________________________")
    print(result_train_lst[:1])
    return {'train': result_train_lst}, model
    [{'input_ids': [0, 8, 20, 24, 125, 69, 4, 8, 20, 77, 125, 84, 4, 6, 20, 
                    88, 71, 129, 4, 6, 75, 88, 125, 129, 4, 6, 41, 21, 
                    104, 23, 4, 6, 20, 73, 125, 77, 1, 3, 3, 3, 3, 3, 3, 3, 3, 
                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 
    'hidden_states': [[-1.6222010850906372, 1.7222366333007812, 0.8530371785163879, 0.0408187098801136, -0.5346421003341675, 0.8283897042274475, -2.47684907913208, -0.3852204382419586], 


if task_mode == 'e2e-tgt':
    training_data, model = get_corpus_rocstory(data_args, model, seq_length,
                    padding_mode=padding_mode, split=split,
                    load_vocab=load_vocab)
'''
src1_valid.txt ä¸‰ä¸ªæ–‡ä»¶ï¼Œ ç„¶ååˆ†è¯ï¼Œç»Ÿè®¡è¯é¢‘ï¼Œå–è¯é¢‘å¤§äºä¸€å®šé˜ˆå€¼ï¼ˆæˆ‘è®¾ç½®äº†0)çš„ä¸œè¥¿ï¼Œç„¶åä¿å­˜åˆ° vocab.json
        '''
### ä¸‹é¢çš„self.text_datasets å°±æ˜¯  ä¸Šé¢çš„training_data
def __getitem__(self, idx):

    # We are not on a new enough PIL to support the `reducing_gap`
    # argument, which uses BOX downsampling at powers of two first.
    # Thus, we do it by hand to improve downsample quality.
    if self.model_arch != 'conv-unet':
        arr = np.array(self.text_datasets['train'][idx]['hidden_states'],
                       dtype=np.float32)
        if self.eigen_transform  is not None:
            old_shape = arr.shape
            # arr = arr.reshape(1, -1) @ self.eigen_transform
            arr = arr.reshape(1, -1) - self.eigen_transform['mean']
            arr = arr @ self.eigen_transform['map']
            arr = arr.reshape(old_shape)
            
        if hasattr(self.data_args, 'noise_level') and self.data_args.noise_level > 0:
            # print(arr.dtype)
            # print(self.data_args.noise_level, 'using the noise level.')
            arr = arr + self.data_args.noise_level * np.random.randn(*arr.shape).astype(arr.dtype)
            # print(arr.dtype)

        out_dict = {}
        out_dict['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])
        # out_dict['mapping_func'] = self.mapping_func
        if self.data_args.experiment_mode == 'conditional_gen':
            out_dict['src_ids'] = np.array(self.text_datasets['train'][idx]['src_ids'])
            out_dict['src_mask'] = np.array(self.text_datasets['train'][idx]['src_mask'])
        # if self.local_classes is not None:
        #     out_dict["y"] = np.array(self.local_classes[idx], dtype=np.int64)
        # print(arr[0],'arr0')
        return arr, out_dict
 batch, types = next(data) ###- ç›¸å½“äºæ˜¯ä¸€ä¸ªdata_loader ï¼Œdatasetæ˜¯ TextDatasetï¼Œç›¸å½“äºarr, out_dict
```
#### è®­ç»ƒé˜¶æ®µ
train.py
```python
model, diffusion = create_model_and_diffusion(
        **args_to_dict(args, model_and_diffusion_defaults().keys())
    )
model2, tokenizer = load_models(args.modality, args.experiment, args.model_name_or_path, args.in_channel,
                                    args.checkpoint_path, extra_args=args)
 data = load_data_text(...)  ###- è¿™é‡Œçš„data æ˜¯å¦å¯ä»¥æ ¹æ®ä»»åŠ¡åŒºè°ƒæ•´ï¼Œè¿™ä¸ªæ˜¯å…³é”®ï¼Ÿ
TrainLoop(...).run_loop()
```

#### æ¨ç†é˜¶æ®µ
batch_decoded.py<br />model : DiscreteTransformerModel ï¼ˆ69ï¼‰
```python
    # load configurations.
config_path = os.path.join(os.path.split(args.model_path)[0], "training_args.json")

model, diffusion = create_model_and_diffusion(
    **args_to_dict(args, model_and_diffusion_defaults().keys())
)
   model2, tokenizer = load_models(args.modality, args.experiment, args.model_name_or_path, args.in_channel,
                                    os.path.split(args.model_path)[0])




###  model åœ¨è¿™é‡Œç”¨åšæ¨ç†ï¼ŒåŒæ—¶å¯¹äºä¸åŒçš„ä»»åŠ¡çš„è¾“å…¥åšäº†å¤„ç†ã€‚
sample = sample_fn(
    model,
    sample_shape,
    sample_start_step=(T_refine if args.constrained=='refine' else args.diffusion_steps),
    content_token=(model_kwargs['y'] if args.constrained is not None else None),
    multistep=args.multistep,
    constrained=args.constrained,
)
# sample åœ¨å“ªé‡Œç”¨ï¼š
#sample from  sample_fn  ---> gathered_samples,   all_images ---> arrs  ---> arr ---> word_lst

# å°†word_lst æœ‰å…³çš„å†…å®¹ï¼šä¼šæ‰§è¡Œï¼š'written the decoded output to ï¼Ÿ ä¼šçš„ï¼š
# written the decoded output to ../results/generation_outputs/publaynet_lex_pretrained\ungen\publaynet_lex_pretrained.ema_0.9999_400000.pt.samples_-1.0_elem1.txt
# written the decoded output to ../results/generation_outputs/publaynet_lex_pretrained\ungen\publaynet_lex_pretrained.ema_0.9999_400000.pt.samples_-1.0_elem1.json
```


**model çš„è¾“å…¥ä¸è¾“å‡ºï¼š**
```python
print("============ model input and output============")
# if flag == 1 :
#     print(log_z) # torch.Size([20, 139, 121]) torch.Size([20, 139, 121])  batch , 139 = 128+5+5 ,121 æ˜¯tensoré•¿åº¦
#     print(log_x_recon)
#     flag = 0 
#     return 
'''
tensor([[[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],
[-69.0776, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],
[-69.0776, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],
...,
[-69.0776, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],
[-69.0776, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],
[-69.0776,   0.0000,   0.0000,  ..., -69.0776, -69.0776, -69.0776]]],
device='cuda:0')
tensor([[[-1.2212e-14, -2.7718e+01, -2.5994e+01,  ..., -2.5921e+01,
    -2.5921e+01, -2.5921e+01],
    [-4.0051e+01, -2.5973e+01, -2.4939e+01,  ..., -2.7537e+01,
    -2.7537e+01, -2.7537e+01],
    [-3.5370e+01, -2.8573e+01, -3.3060e+01,  ..., -3.3323e+01,
    -3.3323e+01, -3.3323e+01],
    ...,
    [-3.4353e+01, -2.1478e+01, -1.9007e+01,  ..., -2.5094e+01,
    -2.5094e+01, -2.5094e+01],
    [-4.1782e+01, -1.8795e+01, -1.5572e+01,  ..., -3.2441e+01,
    -3.2441e+01, -3.2441e+01],
    [-7.0000e+01, -7.0000e+01, -7.0000e+01,  ..., -7.0000e+01,
    -7.0000e+01, -7.0000e+01]]], device='cuda:0')
    
'''
```
**vocab:**
```python
load_vocab {'START': 0, 'END': 1, 'UNK': 2, 'PAD': 3, '|': 4, 'figure': 5, 'text': 6, 'title': 7, 'table': 8, 'list': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, '10': 20, '11': 21, '12': 22, '13': 23, '14': 24, '15': 25, '16': 26, '17': 27, '18': 28, '19': 29, '20': 30, '21': 31, '22': 32, '23': 33, '24': 34, '25': 35, '26': 36, '27': 37, '28': 38, '29': 39, '30': 40, '31': 41, '32': 42, '33': 43, '34': 44, '35': 45, '36': 46, '37': 47, '38': 48, '39': 49, '40': 50, '41': 51, '42': 52, '43': 53, '44': 54, '45': 55, '46': 56, '47': 57, '48': 58, '49': 59, '50': 60, '51': 61, '52': 62, '53': 63, '54': 64, '55': 65, '56': 66, '57': 67, '58': 68, '59': 69, '60': 70, '61': 71, '62': 72, '63': 73, '64': 74, '65': 75, '66': 76, '67': 77, '68': 78, '69': 79, '70': 80, '71': 81, '72': 82, '73': 83, '74': 84, '75': 85, '76': 86, '77': 87, '78': 88, '79': 89, '80': 90, '81': 91, '82': 92, '83': 93, '84': 94, '85': 95, '86': 96, '87': 97, '88': 98, '89': 99, '90': 100, '91': 101, '92': 102, '93': 103, '94': 104, '95': 105, '96': 106, '97': 107, '98': 108, '99': 109, '100': 110, '101': 111, '102': 112, '103': 113, '104': 114, '105': 115, '106': 116, '107': 117, '108': 118, '109': 119, '110': 120, '111': 121, '112': 122, '113': 123, '114': 124, '115': 125, '116': 126, '117': 127, '118': 128, '119': 129, '120': 130, '121': 131, '122': 132, '123': 133, '124': 134, '125': 135, '126': 136, '127': 137}
```
**ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿ**<br />æ—¢ç„¶ æ ‡ç­¾æ²¡æœ‰åŠ å™ªï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆåæ¥ç”Ÿæˆçš„å†…å®¹å¾ˆä¹±ï¼Ÿ è¯·ä½ å¼„æ¸…æ¥šæ¯ä¸ªé˜¶æ®µçš„åè¾“å…¥ä¸è¾“å‡º<br />1.è¿™å››ä¸ªæ•°å­—ä»£è¡¨çš„æ˜¯bounding boxs çš„ æ¡†çš„ä¿¡æ¯<br />left li, top ti, right ri, and bottom bi coordinates<br />2.input æ˜¯ä»€ä¹ˆï¼Ÿ<br />3.mask ï¼Ÿ<br />4.ä¸åŒçš„ç¬¦å·è¡¨ç¤ºä»€ä¹ˆï¼Ÿ ä»–è¡¨ç¤ºåŠ å…¥ä¸åŒç¨‹åº¦çš„å™ªéŸ³<br />è¿™ä¸‰ä¸ªQä¸ºä»€ä¹ˆæ˜¯çŸ©é˜µï¼Ÿ<br />è¯¥å®éªŒçš„æŒ‡æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ<br />è¯¥å®éªŒçš„ä¸»è¦æ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ<br />ä¸ stable diffusionçš„åŒºåˆ«ï¼Ÿ<br />ä¸LayerOUtGAN çš„åŒºåˆ«ï¼Ÿ<br />çŸ¥è¯†å›é¡¾ï¼š<br />SD: æ­£å‘åŠ å™ªï¼Œåå‘ç”Ÿå›¾<br />æ­£å‘è¿‡ç¨‹ä¸€èˆ¬ä¸è®­ç»ƒæ¨¡å‹ï¼Œåå‘è¿‡ç¨‹éœ€è¦è®­ç»ƒå™ªéŸ³ç”Ÿæˆå™¨ã€‚<br />æ­£å‘æ•°æ®é›†ï¼š<br />æ­£å‘è¿‡ç¨‹ï¼š<br />input: ä¸ºå¤§é‡ä¸åŒå™ªç‚¹å¼ºåº¦çº§åˆ«çš„å›¾åƒ (image + ä¸€ä¸ªçº§åˆ«çš„å™ªéŸ³ï¼‰<br />output: è¯¥å™ªéŸ³å›¾åƒå¯¹åº” çš„ çº§åˆ«å™ªéŸ³<br />è¿™æ ·å¯ä»¥æ„æˆï¼š æ•°æ®å¯¹ï¼Œä½†æ˜¯å¹¶ä¸è¿›è¡Œè®­ç»ƒ<br />åå‘è¿‡ç¨‹ï¼š<br />å®é™…åœ¨è¿‡ç¨‹ä¸­ï¼Œå¹¶ä¸ä¸€å®šæ˜¯ä»ä¸€å¼ å®Œå…¨å™ªéŸ³å›¾ç‰‡æ¼”å˜æˆæ— å™ªéŸ³å›¾ç‰‡ã€‚<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/29540509/1717987549159-7c787e90-168d-48c0-8964-e4234683f044.png#averageHue=%2342403a&clientId=u7e84b3ce-c973-4&from=paste&height=335&id=ub35173ab&originHeight=670&originWidth=1192&originalType=binary&ratio=2&rotation=0&showTitle=false&size=764126&status=done&style=none&taskId=u95b3654c-5888-422a-9a12-5848e51429b&title=&width=596)<br />æ‰€ä»¥æ‰©æ•£è¿‡ç¨‹çš„é£æ ¼ï¼Œå®Œå…¨å–å†³äºåœ¨æ­£å‘è¿‡ç¨‹ä¸­_U-Netï¼Œä¹Ÿå°±æ˜¯è®­ç»ƒæ•°æ®é›†çš„é£æ ¼ã€‚_
## LayoutDiffusion
è¿˜æœ‰ä¸€ç¯‡å›½å†…ç‰ˆçš„ï¼Œä¸ä¸Šè¿°æ˜¯åŒæœŸå·¥ä½œã€‚<br />LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation
# å®éªŒ
![[https://apijoyspace.jd.com/v1/files/XOUQgRmQG4h4nXllimZy/link](https://apijoyspace.jd.com/v1/files/XOUQgRmQG4h4nXllimZy/link)](SD+a3438060-e2d4-4c9a-9736-a72a272b75e1/link 5)<br />labelme: è¿™ä¸ªåˆ†åˆ«æ˜¯å·¦ä¸Šï¼Œå³ä¸‹åæ ‡<br />æ–‡å¿ƒä¸€è¨€çš„è§£é‡Šï¼š<br />bboxesæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªè¾¹ç•Œæ¡†ï¼ˆbounding boxesï¼‰çš„tensorã€‚æ¯ä¸ªè¾¹ç•Œæ¡†ç”±å››ä¸ªæ•°å€¼è¡¨ç¤ºï¼Œé€šå¸¸è¿™äº›æ•°å€¼åˆ†åˆ«ä»£è¡¨ï¼š

1. **x_min**(æˆ–ç§°ä¸ºxminæˆ–left)ï¼šè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„xåæ ‡ï¼ˆæ°´å¹³æ–¹å‘çš„èµ·å§‹ä½ç½®ï¼‰ã€‚
2. **y_min**(æˆ–ç§°ä¸ºyminæˆ–top)ï¼šè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„yåæ ‡ï¼ˆå‚ç›´æ–¹å‘çš„èµ·å§‹ä½ç½®ï¼‰ã€‚
3. **width**ï¼šè¾¹ç•Œæ¡†çš„å®½åº¦ã€‚
4. **height**ï¼šè¾¹ç•Œæ¡†çš„é«˜åº¦ã€‚

è¿™é‡Œè¡¨ç¤ºä»€ä¹ˆæ„æ€ï¼Ÿ æ€ä¹ˆè®¡ç®—ï¼Ÿ çœ‹è®ºæ–‡ï¼Ÿ<br />![[https://apijoyspace.jd.com/v1/files/dLP5ewwzdRDr4QZE8fRi/link](https://apijoyspace.jd.com/v1/files/dLP5ewwzdRDr4QZE8fRi/link)](SD+a3438060-e2d4-4c9a-9736-a72a272b75e1/link 6)<br />training set's prior distribution. F<br />python scripts/batch_decode.py ../results/checkpoint/rico_1 -1.0 ema 20 3728 False -1 type<br />python scripts/train.py --checkpoint_path ../results/checkpoint/rico_2 --model_arch transformer --modality e2e-tgt --save_interval 500 --lr 1e-5 --batch_size 4 --diffusion_steps 200 --noise_schedule gaussian_refine_pow2.5 --use_kl False --learn_sigma False --aux_loss True --rescale_timesteps False --seq_length 121 --num_channels 128 --seed 102 --dropout 0.1 --padding_mode pad --experiment random --lr_anneal_steps 20000 --weight_decay 0.0 --predict_xstart True --training_mode discrete1 --vocab_size 186 --submit False --e2e_train ../data/processed_datasets/RICO_ltrb_lex --alignment_loss False<br />python eval_src/tools/draw_from_results.py -d rico -p results/generation_outputs/rico_1/ungen/processed.pt -s results/generation_outputs/rico_1/ungen/pics -n 100
